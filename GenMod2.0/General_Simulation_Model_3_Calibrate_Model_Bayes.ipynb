{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calibration of General Simulation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements a Bayesian approach to finding model parameters that seem reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model specific variables are imported in the file gsm_metadata.pkl. This file is a Python dictionary that was created when the model was created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T18:50:46.229442Z",
     "start_time": "2021-12-29T18:50:44.187142Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import os\n",
    "import datetime as dt\n",
    "import pickle, joblib\n",
    "\n",
    "\n",
    "# Standard data science libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import scipy.optimize as so \n",
    "import scipy.interpolate as si\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-notebook')\n",
    "\n",
    "# Options for pandas\n",
    "pd.options.display.max_columns = 20\n",
    "pd.options.display.max_rows = 200\n",
    "\n",
    "# Display all cell outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import Math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T18:50:48.304564Z",
     "start_time": "2021-12-29T18:50:46.231256Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from matplotlib import colors\n",
    "import flopy as fp\n",
    "from bayes_opt import BayesianOptimization, UtilityFunction # for Bayesian optimization\n",
    "import itertools #efficient creation of iterable sets of model parameters\n",
    "from tqdm.notebook import tqdm # for the progress bars\n",
    "import statsmodels.api as sm # for lowess smoothing of plots\n",
    "from scipy.spatial import ConvexHull # to find the Pareto front\n",
    "import shapely # to operate on the parameter space\n",
    "from scipy.spatial.distance import cdist # to operate on parameter space\n",
    "import json\n",
    "import RTD_util6 as rtd_ut # custom module with utilities\n",
    "import warnings\n",
    "import Genmod_Utilities as gmu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calibration uses a Bayesian strategy with Gaussian Processes to sample parameter space.  There is a good explanation (and where the Python package comes from) at \n",
    "\n",
    "https://github.com/fmfn/BayesianOptimization\n",
    "\n",
    "The basic idea is that you set up parameter bounds to be sampled, run the model a bunch of times (\"initial probing\"), and use Gaussian Processes to construct an error hypersurface (\"hyper\" because there can be more than two dimensions to it; the number of dimensions equals the number of parameters) in parameter space. The algorithm then starts to sample this hypersurface, run the model, and update the hypersurface.  The updates are based on the error measure (sum of aquared error in this case).  In other words, the lower the error measure, the more weight that is given to that parameter set when resampling for the next iteration. There is a trade-off in the algorithm between exploring new areas of the hypersurface and honing in on promising areas. There is a hyperparameter that control this trade off. Sampling is done by the \"acquisition function\", of which there several the user can select. Each acquisition function has slightly different ways of controlling the trade off.  The options are in the code and the user can comment out functions that are not being used. The EI function is the default. The Gaussian Process algorithm also has a hyperparameter (alpha) that controls how much the error hypersurface is smoothed between data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T18:50:48.319498Z",
     "start_time": "2021-12-29T18:50:48.306419Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('GenMod_metadata.txt') as json_file:\n",
    "    metadata = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read metadata dictionary that was created when the model was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T18:50:48.334779Z",
     "start_time": "2021-12-29T18:50:48.321253Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src = os.path.join('model_ws', 'gsm_metadata.json')\n",
    "with open(src, 'r') as f:\n",
    "    gsm_metadata = json.load(f)   \n",
    "    \n",
    "from argparse import Namespace\n",
    "meta = Namespace(**gsm_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the GSM that was created over to the scratch directory.  It will be replaced many times during the exploration of parameter space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T18:50:48.597111Z",
     "start_time": "2021-12-29T18:50:48.336710Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists('model_ws/calibration_runs'):\n",
    "    shutil.rmtree('model_ws/calibration_runs')\n",
    "shutil.copytree('model_ws', 'model_ws/calibration_runs')\n",
    "\n",
    "if os.path.exists('optimal_model'):\n",
    "    shutil.rmtree('optimal_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model and extract a few variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T18:50:49.065183Z",
     "start_time": "2021-12-29T18:50:48.598065Z"
    }
   },
   "outputs": [],
   "source": [
    "sim = fp.mf6.MFSimulation.load(sim_name='mfsim.nam', version='mf6', exe_name=metadata['modflow_path'],\n",
    "                               sim_ws='model_ws/calibration_runs', strict=True, verbosity_level=0, \n",
    "                               load_only=None, verify_data=False)\n",
    "\n",
    "model = sim.get_model()\n",
    "\n",
    "dis = model.get_package('dis')\n",
    "top_ar = dis.top.array\n",
    "top = top_ar.ravel()\n",
    "nlay, nrow, ncol = dis.nlay.array, dis.nrow.array, dis.ncol.array\n",
    "delc = dis.delc.array\n",
    "delr = dis.delr.array\n",
    "\n",
    "npf = model.get_package('npf')\n",
    "k = npf.k.array\n",
    "k33 = npf.k33.array\n",
    "\n",
    "tmp = np.load(os.path.join('bedrock_flag_array.npz'))\n",
    "bedrock_index = tmp['bedrock_index']\n",
    "\n",
    "print ('   ... done') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model_grid.csv file to get the observation cell types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T18:50:49.143425Z",
     "start_time": "2021-12-29T18:50:49.066530Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_file = os.path.join(metadata['gis_dir'], 'model_grid.csv')\n",
    "model_grid = pd.read_csv(model_file)\n",
    "model_grid.fillna(0, inplace=True)\n",
    "\n",
    "model_grid.loc[model_grid[meta.K_bdrk] == 0, meta.ibound] = 0\n",
    "model_grid.loc[model_grid[meta.K_surf] == 0, meta.ibound] = 0\n",
    "\n",
    "model_grid.loc[model_grid.ibound == 0, 'obs_type'] = np.nan\n",
    "\n",
    "topo_cells = model_grid.obs_type == 'topo'\n",
    "hydro_cells = model_grid.obs_type == 'hydro'\n",
    "\n",
    "num_topo = model_grid.obs_type.value_counts()['topo']\n",
    "num_hydro = model_grid.obs_type.value_counts()['hydro']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some optimizer parameters. \n",
    "\n",
    "* **pbounds**: the lower and upper limit for each parameter within which to search\n",
    "* **acq**: the acquisition function for updating Bayes estimates.  Either Upper Confidence Bounds (ucb) or expected improvement (ei)\n",
    "* **kappa, xi**: metaparameter for ucb or ei respectively. Lower values favor exploiting local maxima; higher values favor a broader exploration of parameter space. The range given are only suggestions based on the source web site.\n",
    "* **alpha**: a parameter that can be passed to the underlying Gaussian Process\n",
    "* **dif_wt**: factor used to weight the difference between hydro and topo errors\n",
    "* **hyd_wt**: factor used to weight hydro errors\n",
    "* **num_init**: the number of grid search parameter sets to start the Bayesian sampling.  Total number of sets is (number of parameters) $^{numinit}$\n",
    "* **num_bayes**: the number of Bayesian updates to try\n",
    "\n",
    "ranges of all hyperparameters can be used to automate tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T18:50:49.159074Z",
     "start_time": "2021-12-29T18:50:49.146022Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parameter bounds in native units\n",
    "pbounds = {'k_surf_mult': (-1., 2.), 'k_bdrk_mult': (-1., 2.), 'stream_mult': (\n",
    "    -1., 1.), 'k_bottom_fraction': (-2., 1.)}\n",
    "num_pars = len(pbounds)\n",
    "\n",
    "# select level of feedback from optimizer \n",
    "verbosity = 1\n",
    "\n",
    "# select acquisition function and its parameter\n",
    "acq = 'ei' # or 'ucb'\n",
    "# xi used with ei (0, 0.1)\n",
    "xi = 0.1\n",
    "# kappa used with ucb (1, 10):\n",
    "kappa = 0\n",
    "\n",
    "# select metaparameters for the Gaussian process\n",
    "# higher alpha means more toleration for noise, i.e. more flexibility\n",
    "# alpha = 1.E-10\n",
    "# alpha = 0.001\n",
    "# alpha = 0.5\n",
    "alpha = 3/2\n",
    "# alpha = 5/2\n",
    "\n",
    "# select weights for objective function components\n",
    "# dif_wt based on average topo and hydro errors in Starn et al. 2021 is 24.\n",
    "dif_wt = 1.\n",
    "hyd_wt = 1.\n",
    "\n",
    "# select number of initial samples and Bayesian samples\n",
    "num_init = 50\n",
    "num_Bayes = 200\n",
    "\n",
    "# calculate arrays of initial values to probe\n",
    "parameter_sets = np.empty((num_init, num_pars))\n",
    "\n",
    "parameter_sets[:, 0] = np.random.uniform(*pbounds['k_surf_mult'], num_init)\n",
    "parameter_sets[:, 1] = np.random.uniform(*pbounds['k_bdrk_mult'], num_init)\n",
    "parameter_sets[:, 2] = np.random.uniform(*pbounds['stream_mult'], num_init)\n",
    "parameter_sets[:, 3] = np.random.uniform(*pbounds['k_bottom_fraction'], num_init)\n",
    "\n",
    "# select discrete hyperparameter values for model tuning\n",
    "hp_list = list()\n",
    "\n",
    "# alpha_range = (0.001, 3/2)\n",
    "alpha_range = 0.001\n",
    "\n",
    "try:\n",
    "    ar = len(alpha_range)\n",
    "except:\n",
    "    ar = 1\n",
    "hp_list.append(alpha_range)\n",
    "    \n",
    "hyd_wt_range = (1)\n",
    "\n",
    "try:\n",
    "    ah = len(hyd_wt_range)\n",
    "except:\n",
    "    ah = 1\n",
    "hp_list.append(hyd_wt_range)\n",
    "    \n",
    "xi_range = (0, 0.1)\n",
    "xi_range = (0)\n",
    "\n",
    "try:\n",
    "    ax = len(xi_range)\n",
    "except:\n",
    "    ax = 1\n",
    "hp_list.append(xi_range)\n",
    "\n",
    "num_hyper = ar + ah + ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to update parameter values, run the model, and calculate hydro and topo errors. The parameters of the model are multiplers of the the original values. Parameter multipliers are sampled in log space, so a multiplier of 1 means that the parameter value is 10 times the original value. \n",
    "\n",
    "$K$ for each application of the following function is calculated from the $k$ (designated lower case $k$ in the code, but it refers to hydraulic conductivity, not intrinsic permeability) that was read in in the base model. There are 3 $K$ multipliers that will be optimized and which apply to two hydrogeologic materials--consolidated and unconsolidated. One multiplier (`k_surf_mult`) multiplies $K$ in the unconsolidated (surficial) material. Two multipliers apply to the consolidated (bedrock) material. One of these multipliers (`k_bdrk_mult`) multiplies the uppermost bedrock layer $K$, the other multiplier (`k_bottom_fraction`) multiplies the lowermost bedrock layer. Any bedrock layers in between these two layers is calculated using an exponetial relationship between the top and bottom bedrock $K$ values.  \n",
    "\n",
    "In the previous notebook, layers were created parallel to the simulated water table. By doing this, some cells in a layer may be composed of bedrock while other cells in the same cold be composed of surfiical material. The array created in the previous notebook called `bedrock_index` contains flags that indicate which K should be applied to each cell, surficial $K$ or bedrock $K$.\n",
    "\n",
    "To summarize, a 3D array of bedrock $K$ is calculated from $\\mathbf{k}$ using multipliers and exponential interpolation (decay with depth). Another array is created (same shape as the bedrock array) of the surficial $K$, even though there is no variation with depth. The final $K$ array is made by choosing one of these array for cell based on `bedrock_index`.\n",
    "\n",
    "$K_{top\\_of\\_ bedrock} = \\mathbf{k} * {k\\_bdrk\\_mult}$\n",
    "\n",
    "$K_{layer\\_n} = c e^{a z}$\n",
    "\n",
    "$K_{bottom\\_of\\_ bedrock} = \\mathbf{k} * {k\\_bottom\\_fraction}$\n",
    "\n",
    "where the coefficients $a$ and $c$ are determined in the code from the the top and bottom layer elevations and $K$\n",
    "\n",
    "Streambed $K$ is set as a fraction of the cell $K$.  This parameter is `stream_mult`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that there should be at least 2 bedrock layers for this interpolation to work**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to apply the multipliers and run the model. The effect of the new parameters on streambed permeability is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T18:50:49.190935Z",
     "start_time": "2021-12-29T18:50:49.160982Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_model(k_surf_mult, k_bdrk_mult, stream_mult, k_bottom_fraction, sim_ws='model_ws/calibration_runs'):\n",
    "    \n",
    "#   transform the log multipliers to real multipliers\n",
    "    k_surf_mult = 10 ** k_surf_mult\n",
    "    k_bdrk_mult = 10 ** k_bdrk_mult\n",
    "    stream_mult = 10 ** stream_mult\n",
    "    k_bottom_fraction = 10 ** k_bottom_fraction    \n",
    "    \n",
    "    # use flopy to read in the model\n",
    "    sim = fp.mf6.MFSimulation.load(sim_name='mfsim.nam', version='mf6', \n",
    "                                   exe_name=metadata['modflow_path'],\n",
    "                                   sim_ws=sim_ws, strict=True, verbosity_level=0, \n",
    "                                   load_only=None, verify_data=False)\n",
    "    model = sim.get_model()\n",
    "    dis = model.get_package('dis')\n",
    "    npf = model.get_package('npf')\n",
    "\n",
    "    # set K in each layer\n",
    "    k_top_of_bedrock = k[-gsm_metadata['num_bdrk_layers']] * k_bdrk_mult\n",
    "    k_bottom_of_bedrock = k[-1, ...] * k_bottom_fraction\n",
    "\n",
    "    grid = np.empty((nlay+1, nrow, ncol))\n",
    "    grid[0, ...] = dis.top.array\n",
    "    grid[1:, ...] = dis.botm.array\n",
    "    z = (grid[0:-1, ...] + grid[1:, ...] ) / 2\n",
    "\n",
    "    a = np.log(k_bottom_of_bedrock / k_top_of_bedrock) / (z[-1 , ...] - z[-gsm_metadata['num_bdrk_layers']])\n",
    "    c = k_top_of_bedrock * np.exp(-a * z[-gsm_metadata['num_bdrk_layers']])\n",
    "    k_exp = c * np.exp(a * z)\n",
    "\n",
    "    new_k = np.where(bedrock_index, k_exp, k_surf_mult * k)\n",
    "    npf.k = new_k\n",
    "\n",
    "    model_grid[meta.K_surf] = new_k[0, ...].ravel()\n",
    "    \n",
    "    # set drain data in each drain cell\n",
    "    drn_data = model_grid[(model_grid.order != 0) &\n",
    "                          (model_grid[meta.ibound] == 1)].copy()\n",
    "\n",
    "    # adjust streambed K based on cell K and stream_mult\n",
    "    drn_data['dcond'] = drn_data[meta.K_surf] * stream_mult * \\\n",
    "        drn_data.reach_len * drn_data.width / meta.stream_bed_thk\n",
    "    drn_data['iface'] = 6\n",
    "    drn_data = drn_data.reindex(\n",
    "        ['lay', 'row', 'col', 'stage', 'dcond', 'iface'], axis=1)\n",
    "    drn_data.rename(columns={'lay': 'k', 'row': 'i',\n",
    "                             'col': 'j', 'stage': 'stage'}, inplace=True)\n",
    "    drn_data = drn_data[drn_data.dcond > 0]\n",
    "    \n",
    "    cellid = list(zip(drn_data.k, drn_data.i, drn_data.j))\n",
    "\n",
    "    drn_data6 = pd.DataFrame({'cellid': cellid, 'stage': drn_data.stage,\n",
    "                              'dcond': drn_data.dcond, 'iface': drn_data.iface})\n",
    "    drn_recarray6 = drn_data6.to_records(index=False)\n",
    "    drn_dict6 = {0: drn_recarray6}\n",
    "\n",
    "    drn = model.get_package('drn')\n",
    "    drn.stress_period_data = drn_dict6\n",
    "\n",
    "    # run the model \n",
    "    sim.write_simulation()\n",
    "    sim.run_simulation(silent=True)\n",
    "\n",
    "    # calculate the errors\n",
    "    rtd = rtd_ut.RTD_util(sim, 'flow', 'rt')\n",
    "    rtd.get_watertable()\n",
    "    water_table = rtd.water_table\n",
    "\n",
    "    t_crit = (model_grid.obs_type =='topo') & (model_grid[meta.ibound] != 0)\n",
    "    topo_cells = t_crit.values.reshape(nrow, ncol)\n",
    "\n",
    "    h_crit = (model_grid.obs_type =='hydro') & (model_grid[meta.ibound] != 0)\n",
    "    hydro_cells = h_crit.values.reshape(nrow, ncol)\n",
    "\n",
    "    num_topo = np.count_nonzero(topo_cells)\n",
    "    num_hydro = np.count_nonzero(hydro_cells)\n",
    "\n",
    "    topo = (top_ar + meta.err_tol) < water_table\n",
    "    hydro = (top_ar - meta.err_tol) > water_table\n",
    "\n",
    "    topo_error = topo & topo_cells\n",
    "    hydro_error = hydro & hydro_cells\n",
    "\n",
    "    t = np.count_nonzero(topo_error)\n",
    "    h = np.count_nonzero(hydro_error)\n",
    "\n",
    "    topo_rate = t / num_topo\n",
    "    hydro_rate = h / num_hydro\n",
    "\n",
    "    return topo_rate, hydro_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loops to first run a grid search to initiate the Bayesian sampling, then loop for Bayes. The first commented-out loop can be used to automate hyperparameter tuning. The model may not run for some combinations of parameters. These will be printed out at the bottom of the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T19:13:07.797249Z",
     "start_time": "2021-12-29T18:50:49.191891Z"
    }
   },
   "outputs": [],
   "source": [
    "results_dict = dict()\n",
    "# hyper_parameter_set = itertools.product(*hp_list)\n",
    "# for alpha, xi in tqdm(hyper_parameter_set, total=num_hyper, desc='hyperparameter loop'):\n",
    "# for alpha, xi in tqdm(hp_list, total=num_hyper, desc='hyperparameter loop'):\n",
    "topo_error_list = list()\n",
    "hydro_error_list = list()\n",
    "dif_list = list()\n",
    "sum_list = list()\n",
    "\n",
    "alpha, hyd_wt, xi = hp_list\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"future warning\", FutureWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "    for i in range(1):\n",
    "        dict_key = 'alpha={}_dif_wt={}_xi={}'.format(alpha, hyd_wt, xi)\n",
    "\n",
    "        utility = UtilityFunction(kind=acq, xi=xi, kappa=kappa)\n",
    "        optimizer = BayesianOptimization(\n",
    "            run_model, pbounds=pbounds, verbose=verbosity)\n",
    "        optimizer.set_gp_params(**{'alpha': alpha})\n",
    "\n",
    "        for i in tqdm(parameter_sets, total=num_init, desc='initial probing'):\n",
    "            next_point_to_probe = dict(\n",
    "                (zip(('k_surf_mult', 'k_bdrk_mult', 'stream_mult', 'k_bottom_fraction'), i)))\n",
    "\n",
    "            try:\n",
    "                topo_rate, hydro_rate = run_model(**next_point_to_probe)\n",
    "                edif = dif_wt * np.abs(topo_rate - hydro_rate)\n",
    "                esum = topo_rate + hyd_wt * hydro_rate\n",
    "                target = -(edif + esum)\n",
    "\n",
    "                optimizer.register(\n",
    "                    params=next_point_to_probe,\n",
    "                    target=target)\n",
    "\n",
    "                topo_error_list.append(topo_rate)\n",
    "                hydro_error_list.append(hydro_rate)\n",
    "                dif_list.append(edif)\n",
    "                sum_list.append(esum)\n",
    "            except OSError:\n",
    "                print('model did not run for {}'.format(next_point_to_probe))\n",
    "\n",
    "\n",
    "        for n in tqdm(range(num_Bayes), desc='Bayesian sampling'):\n",
    "            next_point = optimizer.suggest(utility)\n",
    "            \n",
    "            try:\n",
    "                topo_rate, hydro_rate = run_model(**next_point)\n",
    "                edif = dif_wt * np.abs(topo_rate - hydro_rate)\n",
    "                esum = topo_rate + hyd_wt * hydro_rate\n",
    "                target = -(edif + esum)\n",
    "\n",
    "                optimizer.register(params=next_point, target=target)\n",
    "\n",
    "                topo_error_list.append(topo_rate)\n",
    "                hydro_error_list.append(hydro_rate)\n",
    "                dif_list.append(edif)\n",
    "                sum_list.append(esum)\n",
    "            except OSError:\n",
    "                print('model did not run for {}'.format(next_point))\n",
    "\n",
    "\n",
    "df = pd.DataFrame(optimizer.res)\n",
    "df = pd.concat((df, df.params.apply(pd.Series)),\n",
    "               axis=1).drop('params', axis='columns')\n",
    "df['topo_error'] = topo_error_list\n",
    "df['hydro_error'] = hydro_error_list\n",
    "df['dif_error'] = dif_list\n",
    "df['sum_error'] = sum_list\n",
    "results_dict[dict_key] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find one set of the optimal parameters by considering where the Pareto (tradeoff) front between hydro and topo errors intersects the line of hydro error = topo error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T19:13:07.860101Z",
     "start_time": "2021-12-29T19:13:07.798245Z"
    }
   },
   "outputs": [],
   "source": [
    "# find the convex hull of the points in error space\n",
    "ch = ConvexHull(df[['topo_error', 'hydro_error']])\n",
    "\n",
    "# make a polygon of the convex hull\n",
    "hull = df.iloc[ch.vertices]\n",
    "shapely_poly = shapely.geometry.Polygon(hull[['topo_error', 'hydro_error']].values)\n",
    "\n",
    "# make a line of hydro error = topo error\n",
    "line = [(0, 0), (1, 1)]\n",
    "shapely_line = shapely.geometry.LineString(line)\n",
    "\n",
    "# intersect the polygon and the line\n",
    "intersection_line = list(shapely_poly.intersection(shapely_line).coords)\n",
    "\n",
    "# the intersection will occur at two points; use the minimum\n",
    "a = intersection_line[np.array(intersection_line)[:, 0].argmin()]\n",
    "a = np.array(a).reshape(1, 2)\n",
    "\n",
    "b = ch.points\n",
    "\n",
    "# find the distance between all the parameter sets and the point of intersection\n",
    "df['cdist'] = cdist(a, b)[0, :]\n",
    "\n",
    "# find the closest (least distance) parameter set to the intersection\n",
    "crit = df.cdist.idxmin()\n",
    "\n",
    "# extract the optimal parameters and save\n",
    "o = df.iloc[crit]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model using the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T19:13:12.365406Z",
     "start_time": "2021-12-29T19:13:07.861104Z"
    }
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "    topo, hydro = run_model(o.k_surf_mult, o.k_bdrk_mult,\n",
    "                        o.stream_mult, o.k_bottom_fraction, sim_ws='model_ws/calibration_runs')\n",
    "    \n",
    "model = sim.get_model()\n",
    "\n",
    "dis = model.get_package('dis')\n",
    "top = dis.top.array\n",
    "botm = dis.botm.array\n",
    "nlay, nrow, ncol = dis.nlay.array, dis.nrow.array, dis.ncol.array\n",
    "delc = dis.delc.array\n",
    "delr = dis.delr.array\n",
    "ibound = dis.idomain.array\n",
    "\n",
    "npf = model.get_package('npf')\n",
    "k = npf.k.array\n",
    "k33 = npf.k33.array\n",
    "\n",
    "shutil.copytree('model_ws/calibration_runs', 'optimal_model')\n",
    "\n",
    "dst = os.path.join('optimal_model', 'final_k.npz')\n",
    "np.savez(dst, k=k)\n",
    "\n",
    "dst = os.path.join('optimal_model', 'results_df.csv')\n",
    "df.to_csv(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T19:13:12.381528Z",
     "start_time": "2021-12-29T19:13:12.366411Z"
    }
   },
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame({'log multiplier': df[['k_bdrk_mult', 'k_bottom_fraction',\n",
    "       'k_surf_mult', 'stream_mult', 'topo_error', 'hydro_error', 'dif_error',\n",
    "       'sum_error', 'cdist']].iloc[crit]})\n",
    "final_df['transformed'] = np.nan\n",
    "\n",
    "final_df.loc[['k_bdrk_mult', 'k_bottom_fraction',\n",
    "       'k_surf_mult', 'stream_mult'], 'transformed'] = 10 ** final_df.loc[['k_bdrk_mult', 'k_bottom_fraction',\n",
    "       'k_surf_mult', 'stream_mult'], 'log multiplier']\n",
    "\n",
    "final_df.loc['stream_mult', 'transformed'] = final_df.loc['stream_mult', 'transformed'] * gsm_metadata['stream_bed_kadjust']\n",
    "\n",
    "dst = os.path.join('optimal_model', 'best_pars.csv')\n",
    "final_df.to_csv(dst) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To evaluate uncertainty\n",
    "\n",
    "Find the Pareto front where there is a tradeoff between hydro and topo errors. To do this, we must separate the two halves of the convex hull polygon.  We only want the minimum.  Do this by creating a vertical line at each point along the front (which will be at a convex hull node) and taking the minimum.  Assemble the minima into a line shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample the Pareto front, creating points at equal distances along the front and finding the parameter sets that are closest to them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T19:13:12.397365Z",
     "start_time": "2021-12-29T19:13:12.382366Z"
    }
   },
   "outputs": [],
   "source": [
    "test = list()\n",
    "\n",
    "for x in shapely_poly.exterior.xy[0]:\n",
    "    line = [(x, 0), (x, 1)]\n",
    "    points = np.array(shapely.geometry.LineString(line).intersection(shapely_poly).coords)\n",
    "    ok = points.argmin(axis=0)\n",
    "    test.append(tuple(points[ok[1], :]))\n",
    "test = np.unique(test, axis=0)\n",
    "front = shapely.geometry.LineString(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell can be used (all uncommented) to find parmeter sets that lie on the Pareto front. The model can be run for each of these sets to evaluate uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T19:13:12.412582Z",
     "start_time": "2021-12-29T19:13:12.399320Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# the fractional points to sample\n",
    "g = np.linspace(0, 1, 11)\n",
    "b = ch.points\n",
    "\n",
    "pareto_sets = list()\n",
    "# for each point\n",
    "for i in g:\n",
    "    # interpolate its position along the front\n",
    "    x, y = front.interpolate(i, normalized=True).xy\n",
    "    # put that point in an array\n",
    "    a = np.array([[x[0], y[0]]])\n",
    "    # find the closest parameter set to that point and add its index to the list\n",
    "    pareto_sets.append(cdist(a, b).argmin())\n",
    "\n",
    "dst = os.path.join('optimal_model', 'pareto_sets.csv')\n",
    "pareto_df = pd.DataFrame({'df_index': pareto_sets})\n",
    "pareto_df.to_csv(dst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an outer loop was used for hyperparameter tuning, save or load the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T19:13:12.428609Z",
     "start_time": "2021-12-29T19:13:12.413360Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dst = os.path.join('optimal_model', 'results_dict_v2.joblib')\n",
    "# with open(dst, 'wb') as f:\n",
    "#     joblib.dump(results_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T19:13:12.444167Z",
     "start_time": "2021-12-29T19:13:12.429352Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# src = os.path.join('optimal_model', 'results_dict_v2.joblib')\n",
    "# with open(src, 'rb') as f:\n",
    "#     results_dict = joblib.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all points in parameter space and the Pareto front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T19:13:12.945540Z",
     "start_time": "2021-12-29T19:13:12.445121Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(11, 8.5), sharey=True,\n",
    "                       gridspec_kw={'hspace': 0.0, 'wspace': 0})\n",
    "\n",
    "axs = axs.ravel()\n",
    "\n",
    "li = ['k_surf_mult', 'k_bdrk_mult', 'stream_mult', 'k_bottom_fraction']\n",
    "letter = ['A.', 'B.', 'C.', 'D.']\n",
    "\n",
    "for num in range(4):\n",
    "    plot = axs[num]\n",
    "    var = li[num]\n",
    "    im = plot.hexbin(df.topo_error, df.hydro_error, df[var], 30, reduce_C_function=np.mean,\n",
    "                   cmap=plt.cm.nipy_spectral, alpha=0.8, edgecolors='None')\n",
    "\n",
    "    pos = plot.get_position()\n",
    "    cbaxes = fig.add_axes([pos.x0+0.05, pos.y0+0.35, pos.width - 0.1, 0.02]) \n",
    "    cb = plt.colorbar(im, ax=plot, cax=cbaxes, orientation='horizontal')  \n",
    "    dum = fig.text(0.02, 0.5, 'topographic error', rotation='vertical', ha='left', va='center', fontsize=12)\n",
    "    dum = fig.text(0.50, 0.02, 'hydrologic error', rotation='horizontal', ha='center', va='bottom', fontsize=12)\n",
    "    dum = fig.text(pos.x0+0.20, pos.y0+0.28, var, rotation='horizontal', ha='center', va='bottom', fontsize=12)\n",
    "    dum = fig.text(pos.x0+0.02, pos.y0+0.35, letter[num], rotation='horizontal', ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "    dum = plot.plot((0, 1), (0, 1), linestyle='dashed', color='black', linewidth=0.7)\n",
    "    dum = plot.plot(*front.xy, linestyle='dashed', color='black', linewidth=1.5)\n",
    "    dum = plot.grid(False)\n",
    "    dum = fig.suptitle('pareto front')\n",
    "\n",
    "dst = os.path.join('optimal_model', 'pareto_plot.png')\n",
    "plt.savefig(dst)\n",
    "    \n",
    "Image(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T19:13:13.554151Z",
     "start_time": "2021-12-29T19:13:12.946642Z"
    }
   },
   "outputs": [],
   "source": [
    "l, r, c = np.indices((nlay, nrow, ncol))\n",
    "\n",
    "hin = np.argmax(np.isfinite(bedrock_index), axis=0)    \n",
    "bedrock_top =  np.squeeze(botm[hin, r[0,:,:], c[0,:,:]])\n",
    "\n",
    "NROW = nrow\n",
    "NCOL = ncol\n",
    "\n",
    "def ma2(data2D):\n",
    "    return np.ma.MaskedArray(data2D, mask=(ibound[0, ...] == 0))\n",
    "\n",
    "\n",
    "def ma3(data3D):\n",
    "    return np.ma.MaskedArray(data3D, mask=(ibound == 0))\n",
    "\n",
    "\n",
    "def interpolate_travel_times(points, values, xi):\n",
    "    return si.griddata(points, values, xi, method='linear')\n",
    "\n",
    "\n",
    "def plot_travel_times(ax, x, y, tt, shp):\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        return ax.contourf(x.reshape(shp), y.reshape(shp), tt[:].reshape(shp),\n",
    "                           colors=colors, alpha=1.0, levels=levels, antialiased=True)\n",
    "\n",
    "row_to_plot = np.int32(NROW / 2)\n",
    "# row_to_plot = 65\n",
    "xplot = np.linspace(delc[0] / 2, NCOL * delc[0] - delc[0] / 2, NCOL)\n",
    "\n",
    "mKh = ma3(k)\n",
    "mtop = ma2(top.reshape(nrow, ncol))\n",
    "mbed = ma2(bedrock_top)\n",
    "mbot = ma3(botm)\n",
    "\n",
    "rtd = rtd_ut.RTD_util(sim, 'flow', 'rt')\n",
    "rtd.get_watertable()\n",
    "water_table = rtd.water_table\n",
    "\n",
    "# make a color map of fixed colors\n",
    "cmap = plt.cm.coolwarm\n",
    "bounds = [0, 5, 10]\n",
    "norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "fig = plt.figure(figsize=(11, 8.5))\n",
    "\n",
    "ax1 = plt.subplot2grid((4, 1), (0, 0), rowspan=3)\n",
    "\n",
    "dum = ax1.plot(xplot, mtop[row_to_plot, ],\n",
    "               label='land surface', color='black', lw=0.5)\n",
    "dum = ax1.plot(xplot, rtd.water_table[row_to_plot, ],\n",
    "               label='water table', color='blue', lw=1.)\n",
    "dum = ax1.fill_between(xplot, mtop[row_to_plot, ], mbot[0, row_to_plot, :], alpha=0.25,\n",
    "                       color='blue', lw=0.75)\n",
    "for lay in range(nlay-1):\n",
    "    label = 'layer {}'.format(lay+2)\n",
    "    dum = ax1.fill_between(xplot, mbot[lay, row_to_plot, :], mbot[lay+1, row_to_plot, :],\n",
    "                           color=cmap(lay / nlay), alpha=0.50, lw=0.75)\n",
    "dum = ax1.plot(xplot, mbed[row_to_plot, :], label='bedrock',\n",
    "               color='red', linestyle='dotted', lw=1.5)\n",
    "dum = ax1.plot(xplot, mbot[-1, row_to_plot, :], color='black',\n",
    "               linestyle='dashed', lw=0.5, label='model bottom')\n",
    "\n",
    "dum = ax1.legend(loc=0, frameon=False, fontsize=10, ncol=1)\n",
    "dum = ax1.set_ylabel('Altitude, in meters')\n",
    "dum = ax1.set_title('Section along row {}'.format(row_to_plot))\n",
    "\n",
    "ax2 = plt.subplot2grid((4, 1), (3, 0))\n",
    "dum = ax2.fill_between(xplot, 0, mKh[0, row_to_plot, :], alpha=0.25, color='blue',\n",
    "                 label='layer 1', lw=0.75, step='mid')\n",
    "dum = ax1.set_xlabel('Distance in meters')\n",
    "dum = ax2.set_yscale('log')\n",
    "dum = ax2.set_ylabel('Hydraulic conductivity\\n in layer 1, in meters / day')\n",
    "\n",
    "line = 'optimal_{}_xs.png'.format(metadata['HUC8_name'])\n",
    "fig_name = os.path.join('optimal_model', line)\n",
    "plt.savefig(fig_name)\n",
    "\n",
    "Image(fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T19:13:13.899002Z",
     "start_time": "2021-12-29T19:13:13.555098Z"
    }
   },
   "outputs": [],
   "source": [
    "grid = os.path.join(metadata['gis_dir'], 'ibound.tif')\n",
    "mtg = gmu.SourceProcessing(np.nan)\n",
    "mtg.read_raster(grid)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(11, 8.5))\n",
    "\n",
    "t_crit = (model_grid.obs_type =='topo') & (ibound[0, ...].ravel() != 0)\n",
    "topo_cells = t_crit.values.reshape(NROW, NCOL)\n",
    "\n",
    "h_crit = (model_grid.obs_type =='hydro') & (ibound[0, ...].ravel() != 0)\n",
    "hydro_cells = h_crit.values.reshape(NROW, NCOL)\n",
    "\n",
    "num_topo = np.count_nonzero(topo_cells)\n",
    "num_hydro = np.count_nonzero(hydro_cells)\n",
    "\n",
    "topo = (top + meta.err_tol) < water_table\n",
    "hydro = (top - meta.err_tol) > water_table\n",
    "\n",
    "topo_error = topo & topo_cells\n",
    "hydro_error = hydro & hydro_cells\n",
    "\n",
    "mask = (ibound[0] == 0) | ~topo_cells\n",
    "mt = np.ma.MaskedArray(topo_cells, mask)\n",
    "cmap = colors.ListedColormap(['green'])\n",
    "im = ax.pcolormesh(mtg.x_edge, mtg.y_edge, mt, cmap=cmap, alpha=0.2, edgecolors=None)\n",
    "mask = (ibound[0] == 0) | ~topo_error\n",
    "mte = np.ma.MaskedArray(topo_error, mask)\n",
    "cmap = colors.ListedColormap(['green'])\n",
    "im = ax.pcolormesh(mtg.x_edge, mtg.y_edge, mte, cmap=cmap, alpha=0.4, edgecolors=None)\n",
    "\n",
    "mask = (ibound[0] == 0) | ~hydro_cells\n",
    "mh = np.ma.MaskedArray(hydro_cells, mask)\n",
    "cmap = colors.ListedColormap(['blue'])\n",
    "im = ax.pcolormesh(mtg.x_edge, mtg.y_edge, mh, cmap=cmap, alpha=0.2, edgecolors=None)\n",
    "mask = (ibound[0] == 0) | ~hydro_error\n",
    "mhe = np.ma.MaskedArray(hydro_error, mask)\n",
    "cmap = colors.ListedColormap(['blue'])\n",
    "im = ax.pcolormesh(mtg.x_edge, mtg.y_edge, mhe, cmap=cmap, alpha=0.6, edgecolors=None)\n",
    "\n",
    "ax.set_aspect(1)\n",
    "\n",
    "dum = fig.suptitle('Default model errors\\n{} model\\nFraction dry drains (blue) {:0.2f}\\n \\\n",
    "                    Fraction flooded cells (green) {:0.2f}'.format( \\\n",
    " metadata['HUC8_name'], hydro_rate, topo_rate))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "line = 'optimal_{}_error_map.png'.format(metadata['HUC8_name'])   #csc\n",
    "fig_name = os.path.join('optimal_model', line)\n",
    "plt.savefig(fig_name)\n",
    "Image(fig_name)\n",
    "\n",
    "mtg.old_raster = topo_error\n",
    "line = os.path.join('optimal_model', 'topo_error.tif')\n",
    "mtg.write_raster(line)\n",
    "\n",
    "\n",
    "mtg.old_raster = hydro_error\n",
    "line = os.path.join('optimal_model', 'hydro_error.tif')\n",
    "mtg.write_raster(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T19:13:14.166155Z",
     "start_time": "2021-12-29T19:13:13.899828Z"
    }
   },
   "outputs": [],
   "source": [
    "k[ibound == 0] = np.nan\n",
    "\n",
    "for layer in range(nlay):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    im = ax.imshow(k[layer, ...])\n",
    "    ax.set_title('K in layer {}'.format(layer))\n",
    "    fig.colorbar(im)\n",
    "    mtg.old_raster = k[layer, ...]\n",
    "    line = os.path.join('optimal_model', 'k_layer_{}.tif'.format(layer))\n",
    "    mtg.write_raster(line)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-29T19:13:14.403189Z",
     "start_time": "2021-12-29T19:13:14.167071Z"
    }
   },
   "outputs": [],
   "source": [
    "rtd = rtd_ut.RTD_util(sim, 'flow', 'rt')\n",
    "rtd.get_watertable()\n",
    "water_table = rtd.water_table\n",
    "\n",
    "water_table[water_table > (2 * model_grid.ned.max())] = np.nan\n",
    "mtg.new_array = water_table\n",
    "\n",
    "fig, ax = mtg.plot_raster(which_raster='new', sk={'figsize': (11, 8.5)})\n",
    "fig.set_tight_layout(True)\n",
    "dst = os.path.join('postcal-heads.tif')\n",
    "\n",
    "dst = os.path.join('postcal-heads.png')\n",
    "\n",
    "plt.savefig(dst)\n",
    "\n",
    "i = Image(filename='postcal-heads.png')\n",
    "i"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {
    "eaf3a954c6e44063805d67090344a922": {
     "views": [
      {
       "cell_index": 6
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
